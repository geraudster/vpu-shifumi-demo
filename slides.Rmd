---
title: "Embarquez vos réseaux de neurones"
subtitle: "À la découverte du Edge Computing avec le Neural Stick Computing"
author: "Géraud Dugé de Bernonville"
date: "26 octobre 2018"
output:
  ioslides_presentation:
    widescreen: true
    logo: slides/red.png
---

# La problématique

> - IoT
> - Cloud
> - Performance
> - Énergie

## {data-background=slides/air-aircraft-clouds-219701.jpg data-background-size=cover}

## {data-background=slides/object-detection-recognition-and-tracking-fig00.jpg data-background-size=cover}

## {data-background=slides/2017-05-03-segmentation11-large.png data-background-size=cover}

## {data-background=slides/Neurons-by-Penn-State.jpg data-background-size=cover}

<div class="notes">

- reconnaissance d'images
- réseaux de neurones

- modèles complexes, l'inférence peut être gourmande en ressources physique (mem + cpu)
- souvent on aimerait que l'inférence soit faite au plus proche du capteur => embarqué
- embarqué => perf => énergie
- réduire les échanges réseaux (avec le cloud)
</div>

# Récap' {data-background=slides/blur-figurine-landscape-1084751.jpg data-background-size=cover}


## Les bases {.smaller}

Perceptron was created in 1957 by Frank Rosenblatt

```{r,Perceptron,out.width='25%', fig.align='center', fig.cap='Perceptron', echo=FALSE}
knitr::include_graphics("slides/artificial_neuron.jpg")
```

* **Input**: $(x_1, x_2, ..., x_n)$ a n-vector
* **Weights:** $(w_1, w_2, ..., w_n)$ a n-vector
* **Bias:** b, a scalar
* **Output:**: 

$y = \begin{cases}
1,  & \text{if $\sum_{i=1}^n x_i.w_i + b > $ threshold} \\
0, & {otherwise}
\end{cases}$

## {data-background=slides/neuralnetworks.png data-background-size=contain}

## Inférence

```{r,MLP_forward_propagation,out.width='25%', fig.align='center', fig.cap='MLP forward propagation', echo=FALSE}
knitr::include_graphics("slides/MLP_forward_1.jpeg")
```
* $x$ vector representing one input data sample, $f$ activation function
* $a^{(1)} = x$
* $z^{(2)} = a^{(1)}.W^{(1)} + b^{(1)} = x.W^{(1)} + b^{(1)}$
* $a^{(2)} = f(z^{(2)})$
* $z^{(3)} = a^{(2)}.W^{(2)} + b^{(2)}$
* $y = a^{(3)} = f(z^{(3)})$


# Edge Computing

> L'edge computing est une méthode d'optimisation employée dans le cloud computing qui consiste à traiter les données à la périphérie du réseau, près de la source des données -- Wikipedia

## VPU {.build}


```{r,Movidius,out.width='70%', fig.align='center', fig.cap='Movidius (acheté par Intel)', echo=FALSE}
knitr::include_graphics("slides/Screenshot_2018-10-24 Intel® Movidius™ Neural Compute Stick.png")
```


```{r,google_aiy,out.width='70%', fig.align='center', fig.cap='Google AIY', echo=FALSE}
knitr::include_graphics("slides/Screenshot_2018-10-24 Edge TPU Devices.png")
```

# Dans le Neural Compute Stick

## {data-background=slides/NCS1_ArchDiagram.jpg data-background-size=cover}

<div class="notes">
- an array of 12 VLIW vector processors called SHAVE processors. These processors are used to accelerate neural networks by running parts of the neural networks in parallel.
- SPARC microprocessor core that runs custom firmware
- A LEON processor coordinates receiving the graph file and images for inference via the USB connection. It also parses the graph file and schedules kernels to the SHAVE neural compute accelerator engines. In addition, the LEON processor also takes care of monitoring die temperature and throttling processing on high temperature alerts
</div>

# Démarche {.build}

- train ton modèle
- profile/tune/compile ton modèle sur PC
- prototype sur système embarqué

# Exemple {data-background=slides/adult-ai-artificial-intelligence-1020325.jpg data-background-size=cover}

# Alphashifumi {data-background=slides/alphashifumi_v1.jpg data-background-size=contain}

## V2 {data-background=slides/alphashifumi_v2.jpg data-background-size=contain}

## V3 {data-background=slides/T800.jpg data-background-size=contain}

# Entrainement du modèle

```{r,set_virtualenv,results='hide',message=FALSE,echo=FALSE}
library(reticulate)
use_virtualenv('~/.virtualenvs/r-tensorflow-py3', required = TRUE)
```
## Chargement des librairies:

```{r,load_libraries,results='hide',message=FALSE}
library(keras)
library(tensorflow)
```

```{r,settings,results='hide',message=FALSE,echo=FALSE}
img_width <- 128
img_height <- 128
train_data_dir <-  "/home/geraud/data/alphashifumi/train"
validation_data_dir <-  "/home/geraud/data/alphashifumi/test"
nb_train_samples <-  4125
nb_validation_samples <-  466 
batch_size <-  50
epochs <-  50
model_output_dir <- './target/models/'
augmented_data <- './target/generated/'

if (!dir.exists(model_output_dir)) dir.create(model_output_dir, recursive = TRUE)
if (!dir.exists(augmented_data)) dir.create(augmented_data, recursive = TRUE)
```

## Modèle simple

```{r,simple_model}
model <- keras_model_sequential()
model %>%
    layer_flatten(input_shape = c(128, 128, 3)) %>%
    layer_dense(units = 1024, activation = 'relu') %>%
    layer_dense(units = 1024, activation = 'relu') %>%
    layer_dense(units = 5, activation = 'softmax', name = 'output')
summary(model)
```
## Compilation

On _compile_ le modèle :

```{r,model_compilation}
model %>% compile(optimizer = 'rmsprop',
                  loss = 'categorical_crossentropy',
                  metrics = c('accuracy'))
```

```{r,datagen,results='hide',echo=FALSE,message=FALSE}
datagen <- image_data_generator(
    rescale = 1./255,
    horizontal_flip = TRUE,
    fill_mode = "nearest",
    zoom_range = 0.3,
    width_shift_range = 0.3,
    height_shift_range = 0.3,
    rotation_range = 30,
    data_format = 'channels_last')

train_generator <- flow_images_from_directory(
    train_data_dir,
    generator = datagen,
    target_size = c(img_height, img_width),
    batch_size = batch_size, 
    class_mode = "categorical",
    save_to_dir = augmented_data)

validation_generator <- flow_images_from_directory(
    validation_data_dir,
    generator = datagen,
    target_size = c(img_height, img_width),
    class_mode = "categorical")
```

## Entrainement

```{r,train_model}
history <- model %>% fit_generator(train_generator,
                                   validation_data = validation_generator,
                                   steps_per_epoch = 5,
                                   validation_steps = 10,
                                   epochs = 5)
```

## Performance

```{r,train_history}
plot(history)
```

## Freeze du modèle

```{r,freeze_model}
source('freeze.R')

io_names <- freeze(model)
```

# Compilation


## Toolkit 

On compile le modèle dans un format pris en charge par le NCS.
Avec :

- `r io_names$input` en entrée
- `r io_names$output` en sortie.
                                       
```{r,set_io_names_env,echo=FALSE}
source('freeze.R')

set_io_names_env(io_names$input, io_names$output)
```

```{bash,ncscompile}
PYTHONPATH="${PYTHONPATH}:/opt/movidius/caffe/python"
source /opt/movidius/virtualenv-python/bin/activate
mkdir -p $PWD/target/ncs

mvNCCompile target/models/keras_to_tf.pb -o target/ncs/ncs.graph \
	       -in $INPUT_NAME \
 	       -on $OUTPUT_NAME \
         -is 128 128
#$
```

# Test avec le NCS

## Liste des devices

```{r,set_virtualenv_ncs,results='hide',message=FALSE,echo=FALSE}
library(reticulate)
use_virtualenv('/opt/movidius/virtualenv-python', required = TRUE)
```

```{python,device_list}
from mvnc import mvncapi

device_list = mvncapi.enumerate_devices()
print(device_list)
```

Initialisation et ouverture d'un device :

```{python,device_open}
device = mvncapi.Device(device_list[0])
device.open()
```

---

Affichage de quelques options :
```{python,device_options}
print(device.get_option(mvncapi.DeviceOption.RO_DEVICE_NAME))
print(device.get_option(mvncapi.DeviceOption.RO_MEMORY_SIZE))
print(device.get_option(mvncapi.DeviceOption.RO_DEVICE_STATE))
```

## Chargement du graphe

```{python,graph_to_buffer}
GRAPH_FILEPATH = './target/ncs/ncs.graph'
with open(GRAPH_FILEPATH, mode='rb') as f:
    graph_buffer = f.read()
```

On utilise la classe _Graph_ :
```{python,init_graph}
graph = mvncapi.Graph('graph1')
```

Les entrées/sorties du graphe sont gérés par des files. On peut les créer avec les paramètres par défaut :
```{python,init_fifo}
input_fifo, output_fifo = graph.allocate_with_fifos(device, graph_buffer)
```

## Inférons 1/2

On charge une image :

```{r,image_sample,out.width='10%', fig.align='center', fig.cap='Exemple shifumi', echo=FALSE}
knitr::include_graphics("data/alphashifumi/test/paper/26475c05-9d1e-11e7-abeb-b1233b68540d-1.jpg")
```

```{python,first_inference}
import cv2
import numpy

tensor = cv2.imread('data/alphashifumi/test/paper/26475c05-9d1e-11e7-abeb-b1233b68540d-1.jpg')
tensor = tensor.astype(numpy.float32)
```

L'image est ensuite poussée dans la file d'entrée :
```{python,queue_inference}
graph.queue_inference_with_fifo_elem(input_fifo, output_fifo, tensor, 'test paper')
```

## Inférons 2/2

L'image va être lue par le device qui va ensuite procéder à l'inférence. Le résultat est ensuite poussé dans la file de sortie :
```{python,read_inference}
output, user_obj = output_fifo.read_elem()
print(output)
print(user_obj)
```


## Nettoyage

```{python,clean_all_1}
input_fifo.destroy()
output_fifo.destroy()
graph.destroy()
device.close()
device.destroy()
```

# Benchmark

## CPU vs. VPU

CPU :
```{r,benchmark_r,echo=FALSE}
img_preproc <- function(img) {
    image_load(img) %>%
        image_to_array
}

img_dir  <- 'data/alphashifumi/test'
imgs <- lapply(list.files(img_dir, recursive = TRUE), function(filename) {
    img_preproc(paste(img_dir, filename, sep = '/')) %>%
        array_reshape(c(1, img_height, img_width, 3))
})

sapply(imgs, function(img) {
    system.time(predict(model, img))['elapsed']
}) %>% summary(digits = 3)
```

```{python,benchmark_ncs,echo=FALSE}
from mvnc import mvncapi

device_list = mvncapi.enumerate_devices()
device = mvncapi.Device(device_list[0])
device.open()
GRAPH_FILEPATH = './target/ncs/ncs.graph'
with open(GRAPH_FILEPATH, mode='rb') as f:
    graph_buffer = f.read()

graph = mvncapi.Graph('graph1')
input_fifo, output_fifo = graph.allocate_with_fifos(device, graph_buffer)

import cv2
import numpy

import os
import time

tensors = [ cv2.imread(os.path.join(dirname, filename)).astype(numpy.float32) for dirname, dirnames, filenames in os.walk('data/alphashifumi/test') for filename in filenames ]
times = []
for tensor in tensors:
    start = time.time()
    graph.queue_inference_with_fifo_elem(input_fifo, output_fifo, tensor, 'test paper')
    output, user_obj = output_fifo.read_elem()
    times.append(time.time() - start)
```

```{python,clean_all_2,echo=FALSE}
input_fifo.destroy()
output_fifo.destroy()
graph.destroy()
device.close()
device.destroy()
```
VPU :
```{r,summary_vpu,echo=FALSE}
summary(py$times, digits = 3)
```

# Transfer Learning

<div class="notes">

## Inception

```{r,inception_loading,eval=FALSE}
base_model <- application_inception_v3(weight = 'imagenet',
                                       include_top = FALSE,
                                       input_shape = c(img_width, img_height, 3))
```

On ajoute nos couches denses :

```{r,custom_layers,eval=FALSE}
predictions <- base_model$output %>%
    layer_global_average_pooling_2d() %>%
    layer_dense(units = 1024, activation = 'relu') %>%
    layer_dense(units = 1024, activation = 'relu') %>%
    layer_dense(units = 5, activation = 'softmax', name = 'output')

```

## On regroupe les morceaux

Le nouveau modèle à entrainer devient :
```{r,keras_model,eval=FALSE}
model <- keras_model(inputs = base_model$input, outputs = predictions)
```

On _freeze_ toutes les couches, pas besoin de les réentrainer :

```{r,freeze_layers,eval=FALSE}
for (layer in base_model$layers)
  layer$trainable <- FALSE
```
</div>

# Liens

- https://software.intel.com/en-us/ai-academy/students/kits/ai-on-the-edge-vision-movidius

# Credits

- Neurons-by-Penn-State.jpg (Photo credit: Penn State)
- 2017-05-03-segmentation11-large.png (https://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html)
- object-detection-recognition-and-tracking-fig00.jpg (https://software.intel.com/en-us/articles/a-closer-look-at-object-detection-recognition-and-tracking)
- neuralnetworks.png (http://www.asimovinstitute.org/neural-network-zoo/)
- 
