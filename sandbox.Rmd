-*- mode: poly-markdown+r -*-
---
title: "An example Knitr/R Markdown document"
author: "Géraud Dugé de Bernonville"
date: "29 septembre 2018"
output: html_document
---

# Installation de Keras

Installation du package:

```{r,results='hide',message=FALSE,cache=TRUE}
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/keras")
##install_keras(method = 'virtualenv', tensorflow = '1.7.0')
```

```{bash}
virtualenv -p python3 ~/.virtualenvs/r-tensorflow-py3
```

```{r,results='hide',message=FALSE,cache=TRUE}
library(reticulate)
##virtualenv_create('r-tensorflow-py3') ## should be created manually
virtualenv_install('r-tensorflow-py3', 'tensorflow==1.7.0')
virtualenv_install('r-tensorflow-py3', 'keras')

```

Installation de tensorflow:

```{r,results='hide',message=FALSE,cache=TRUE}
py_install('pydot', envname = 'r-tensorflow-py3')
```

```{r}
library(keras)
library(reticulate)
use_virtualenv('~/.virtualenvs/r-tensorflow-py3', required = TRUE)
```

# Définition des constantes

```{r}
img_width <- 128
img_height <- 128
train_data_dir <-  "/home/geraud/data/alphashifumi/train"
validation_data_dir <-  "/home/geraud/data/alphashifumi/test"
nb_train_samples <-  4125
nb_validation_samples <-  466 
batch_size <-  50
epochs <-  50
model_output_dir <- './target/models/'
augmented_data <- './target/generated/'

if (!dir.exists(model_output_dir)) dir.create(model_output_dir, recursive = TRUE)
if (!dir.exists(augmented_data)) dir.create(augmented_data, recursive = TRUE)
```

# Keras training

Chargement du modèle MobileNet

```{r}
base_model <- application_mobilenet(weight = 'imagenet',
                                    include_top = FALSE,
                                    input_shape = c(img_width, img_height, 3))

```

On ajoute nos couches denses :

```{r}
predictions <- base_model$output %>%
    layer_global_average_pooling_2d() %>%
    layer_dense(units = 1024, activation = 'relu') %>%
#    layer_dropout(0.5) %>%
    layer_dense(units = 1024, activation = 'relu') %>%
#    layer_dense(units = 1024, activation = 'relu') %>%
#    layer_dense(units = 1024, activation = 'relu') %>%
    layer_dense(units = 5, activation = 'softmax', name = 'output')

```

Le nouveau modèle à entrainer devient :
```{r}
model <- keras_model(inputs = base_model$input, outputs = predictions)
```

On _freeze_ toutes les couches, pas besoin de les réentrainer :

```{r}
for (layer in base_model$layers)
  layer$trainable <- FALSE
#freeze_weights(base_model)
```

On _compile_ le modèle :

```{r}
model %>% compile(optimizer = 'rmsprop',
                  loss = 'categorical_crossentropy',
                  metrics = c('accuracy'))
```

Notre nouveau modèle ressemble à ça :

```{r}
summary(model)
```
# Chargement des données

On configure les générateurs:
```{r}
train_datagen <- image_data_generator(
    rescale = 1./255,
    horizontal_flip = TRUE,
    fill_mode = "nearest",
    zoom_range = 0.3,
    width_shift_range = 0.3,
    height_shift_range = 0.3,
    rotation_range = 30,
    data_format = 'channels_last')

test_datagen <- image_data_generator(
    rescale = 1./255,
    horizontal_flip = TRUE,
    fill_mode = "nearest",
    zoom_range = 0.3,
    width_shift_range = 0.3,
    height_shift_range = 0.3,
    rotation_range = 30,
    data_format = 'channels_last')

```

Chargement des images:

```{r}
train_generator <- flow_images_from_directory(
    train_data_dir,
    generator = train_datagen,
    target_size = c(img_height, img_width),
    batch_size = batch_size, 
    class_mode = "categorical",
    save_to_dir = augmented_data)

validation_generator <- flow_images_from_directory(
    validation_data_dir,
    generator = train_datagen,
    target_size = c(img_height, img_width),
    class_mode = "categorical")

```

# Entrainement !

```{r}
history <- model %>% fit_generator(train_generator,
                                   validation_data = validation_generator,
                                   steps_per_epoch = 5,
                                   validation_steps = 10,
                                   epochs = 5)

```

On peut surveiller la courbe d'apprentissage :

```{r}
plot(history)
```

# Sauvegarde du modèle

```{r}
save_model_hdf5(model, paste0(model_output_dir, 'hdf5'))
```

```{r}
export_savedmodel(model, paste0(model_output_dir, 'savedmodel'))
```

# Freeze du modèle

```{r}
library(keras)
library(tensorflow)

k_clear_session()
sess <- tf$Session()
k_set_session(sess)
model <- load_model_hdf5(paste0(model_output_dir, 'hdf5'))
k_set_learning_phase(0)

library(tensorflow)
model$input_names
model$output_names
output <- model$output$name

#sess <- k_get_session()

optimized_graph <- sess$graph$as_graph_def()

#tf$train$write_graph(optimized_graph, model_output_dir, 'non_optimized.pb', as_text=FALSE)

optimized_graph <- tf$graph_util$convert_variables_to_constants(sess, optimized_graph, list('output/Softmax'))
tf$train$write_graph(optimized_graph, model_output_dir, 'constants_optimized.pb', as_text=FALSE)

optimized_graph <- tf$graph_util$remove_training_nodes(optimized_graph)

tf$train$write_graph(optimized_graph, model_output_dir, 'optimized.pb', as_text=FALSE)
#tf$train$write_graph(optimized_graph, model_output_dir, 'optimized.txt', as_text=TRUE)
```

```{r}
k_clear_session()
model <- load_model_hdf5(paste0(model_output_dir, 'hdf5'))
k_set_learning_phase(0)

library(tensorflow)
model$input_names
model$output_names
model$output[1]$name

sess <- k_get_session()

optimized_graph <- sess$graph$as_graph_def()
transforms <- list(
    'fold_constants(ignore_errors=true)',
    'fold_batch_norms',
    'fold_old_batch_norms',
    "quantize_weights")

#  "quantize_weights", "quantize_nodes",
#  'fold_batch_norms',
#  'fold_old_batch_norms')
#  'quantize_weights']
#transforms = ['merge_duplicate_nodes']
                                        #transforms = []

transformed_graph_def = tf$tools$graph_transforms$TransformGraph(optimized_graph,
                                                                 model$input_names,
                                                                 model$output[1]$name,
                                                                 transforms)

tf$train$write_graph(transformed_graph_def, model_output_dir, 'transformed.pb', as_text=FALSE)

```

```{r}
library(keras)
library(tensorflow)
k_clear_session()
model <- load_model_hdf5(paste0(model_output_dir, 'hdf5'))

k_set_learning_phase(0)

config <- get_config(model)
weights <- get_weights(model)

# re-build a model where the learning phase is now hard-coded to 0
new_model <- from_config(config)
set_weights(new_model, weights)

new_model$input
new_model$output


sess <- k_get_session()
optimized_graph <- sess$graph$as_graph_def()
optimized_graph <- tf$graph_util$convert_variables_to_constants(sess, optimized_graph, list('output_1/Softmax'))


tf$train$write_graph(optimized_graph, model_output_dir, 'keras_to_tf.pb', as_text=FALSE)

```

# Compilation vers le NCS

```{bash}
docker pull geraudster/ncs-container
```

```{bash}
echo docker run -v $PWD/target/models/optimized.pb:/model.pb -it geraudster/ncs-container mvNCCompile /model.pb -in 'input_1' -on 'strided_slice'
```
